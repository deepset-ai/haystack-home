---
title: Haystack 2.20.0-rc2
description: Release notes for Haystack 2.20.0-rc2
toc: True
date: 2025-11-13
last_updated: 2025-11-13
tags: ["Release Notes"]
link: https://github.com/deepset-ai/haystack/releases/tag/v2.20.0-rc2
---

# Release Notes

## v2.20.0-rc2

### Bug Fixes

-   Fixed a bug where components explicitly listed in <span class="title-ref">include_outputs_from</span> would not appear in the pipeline results if they returned an empty dictionary. Now, any component specified in <span class="title-ref">include_outputs_from</span> will be included in the results regardless of whether its output is empty.

## v2.21.0-rc0

### Upgrade Notes

-   Agent and LLMMessagesRouter components now require explicit warm_up() calls before run() when used in standalone mode (outside of a Pipeline) with tools. This change was introduced by PR #9942 which added warm_up() methods to ChatGenerator components for tool initialization.

    **Who is affected:** Users running Agent or LLMMessagesRouter standalone with ChatGenerators that have tools.

    **Action required:** Call warm_up() before run() or run_async():

    ``` python
    # Agent with tools
    agent = Agent(llm=OpenAIChatGenerator(model="gpt-4"), tools=[my_tool])
    agent.warm_up()  # Required in standalone mode
    result = agent.run(messages=[ChatMessage.from_user("Hello")])

    # LLMMessagesRouter with tools
    router = LLMMessagesRouter(generator=OpenAIChatGenerator(model="gpt-4"), tools=[my_tool])
    router.warm_up()  # Required in standalone mode
    result = router.run(messages=[ChatMessage.from_user("Hello")])
    ```

    **Note:** This requirement only applies to standalone usage. When these components are used within a Pipeline, warm_up() is called automatically. We are reconsidering this design choice - see issue #9966 for discussion about automatically calling warm_up() at runtime instead of requiring explicit calls.

### New Features

-   Added the <span class="title-ref">AzureOpenAIResponsesChatGenerator</span>, a new component that integrates Azure OpenAI's Responses API into Haystack. This unlocks several advanced capabilities from the Responses API:

    -   Allowing retrieval of concise summaries of the model's reasoning process.
    -   Allowing the use of native OpenAI or MCP tool formats, along with Haystack Tool objects and Toolset instances.

    Example with reasoning and web search tool: `` `python from haystack.components.generators.chat import AzureOpenAIResponsesChatGenerator from haystack.dataclasses import ChatMessage  chat_generator = AzureOpenAIResponsesChatGenerator(     azure_endpoint="https://example-resource.azure.openai.com/",     azure_deployment="gpt-5-mini",     generation_kwargs={"reasoning": {"effort": "low", "summary": "auto"}}, )  response = chat_generator.run(     messages=[ChatMessage.from_user("What's Natural Language Processing?")] ) print(response["replies"][0].text) ``\`

-   Add an <span class="title-ref">extra</span> field to <span class="title-ref">ToolCall</span> and <span class="title-ref">ToolCallDelta</span> to store provider-specific information.

-   If logprobs are enabled in the generation kwargs, return logprobs in ChatMessage.meta for <span class="title-ref">OpenAIChatGenerator</span> and <span class="title-ref">OpenAIResponsesChatGenerator</span>.

-   Added the OpenAIResponsesChatGenerator, a new component that integrates OpenAI's Responses API into Haystack. This unlocks several advanced capabilities from the Responses API:

    -   Allowing retrieval of concise summaries of the model's reasoning process.
    -   Allowing the use of native OpenAI or MCP tool formats, along with Haystack Tool objects and Toolset instances.

    Example with reasoning and web search tool: `` `python from haystack.components.generators.chat import OpenAIResponsesChatGenerator from haystack.dataclasses import ChatMessage  chat_generator = OpenAIResponsesChatGenerator(     model="o3-mini",     generation_kwargs={         {"summary": "auto", "effort": "low"}     },     tools=[{"type": "web_search"}] )  response = chat_generator.run(     messages=[         ChatMessage.from_user("What's a positive news story from today?")     ] ) print(response["replies"][0].text) ``\`

    Example with structured output: `` `python from pydantic import BaseModel from haystack.components.generators.chat import OpenAIResponsesChatGenerator from haystack.dataclasses import ChatMessage  class WeatherInfo(BaseModel):     location: str     temperature: float     conditions: str  chat_generator = OpenAIResponsesChatGenerator(     model="gpt-5-mini",     generation_kwargs={"text_format": WeatherInfo} )  response = chat_generator.run(     messages=[ChatMessage.from_user("What's the weather in Paris?")] ) ``\`

-   Updated our serialization and deserialization of PipelineSnapshots to work with pydantic BaseModels

-   Added async support to <span class="title-ref">SentenceWindowRetriever</span> with a new <span class="title-ref">run_async()</span> method, allowing the retriever to be used in async pipelines and workflows.

-   Added warm_up() method to all ChatGenerator components (OpenAIChatGenerator, AzureOpenAIChatGenerator, HuggingFaceAPIChatGenerator, HuggingFaceLocalChatGenerator, and FallbackChatGenerator) to properly initialize tools that require warm-up before pipeline execution. The warm_up() method is idempotent and follows the same pattern used in Agent and ToolInvoker components. This enables proper tool initialization in pipelines that use ChatGenerators with tools but without an Agent component.

-   The <span class="title-ref">AnswerBuilder</span> component now exposes a new parameter <span class="title-ref">return_only_referenced_documents</span> (default: True) that controls if only documents referenced in the <span class="title-ref">replies</span> are returned. Returned documents include two new fields in the <span class="title-ref">meta</span> dictionary:

    -   \`source_index\`: the 1-based index of the document in the input list

    \- \`referenced\`: a boolean value indicating if the document was referenced in the <span class="title-ref">replies</span> (only present if the <span class="title-ref">reference_pattern</span> parameter is provided). These additions make it easier to display references and other sources within a RAG pipeline.

### Enhancement Notes

-   Adds <span class="title-ref">generation_kwargs</span> to the <span class="title-ref">Agent</span> component, allowing for more fine-grained control at run-time over the chat generation.
-   Added a <span class="title-ref">revision</span> parameter to all Sentence Transformers embedder components (<span class="title-ref">SentenceTransformersDocumentEmbedder</span>, <span class="title-ref">SentenceTransformersTextEmbedder</span>, <span class="title-ref">SentenceTransformersSparseDocumentEmbedder</span>, and <span class="title-ref">SentenceTransformersSparseTextEmbedder</span>) to allow users to specify a specific model revision/version from the Hugging Face Hub. This enables pinning to a particular model version for reproducibility and stability.
-   Updated the components <span class="title-ref">Agent</span>, <span class="title-ref">LLMMetadataExtractor</span>, <span class="title-ref">LLMMessagesRouter</span>, and <span class="title-ref">LLMDocumentContentExtractor</span> to automatically call <span class="title-ref">self.warm_up()</span> at runtime if they have not been warmed up yet. This ensures that the components are ready for use without requiring an explicit warm-up call. This differs from previous behavior where the warm-up had to be manually invoked before using these components otherwise they would raise a <span class="title-ref">RuntimeError</span>.
-   Improve log-trace correlation for <span class="title-ref">DatadogTracer</span> by using the official <span class="title-ref">ddtrace.tracer.get_log_correlation_context()</span> method.
-   Improved Toolset warm-up architecture for better encapsulation. The base Toolset.warm_up() method now warms up all tools by default, while subclasses can override it to customize initialization (e.g., setting up shared resources instead of warming individual tools). The warm_up_tools() utility function has been simplified to delegate to Toolset.warm_up().

### Bug Fixes

-   Fix deserialization of state schema when it is None in Agent.from_dict.

-   Fixed type compatibility issue where passing <span class="title-ref">list\[Tool\]</span> to components with a <span class="title-ref">tools</span> parameter (such as <span class="title-ref">ToolInvoker</span>) caused static type checker errors. In version 2.19, the <span class="title-ref">ToolsType</span> was changed to <span class="title-ref">Union\[list\[Union\[Tool, Toolset\]\], Toolset\]</span> to support mixing Tools and Toolsets. However, due to Python's list invariance, <span class="title-ref">list\[Tool\]</span> was no longer considered compatible with <span class="title-ref">list\[Union\[Tool, Toolset\]\]</span>, breaking type checking for the common pattern of passing a list of Tool objects.

    The fix explicitly lists all valid type combinations in \`ToolsType\`: <span class="title-ref">Union\[list\[Tool\], list\[Toolset\], list\[Union\[Tool, Toolset\]\], Toolset\]</span>. This preserves backward compatibility for existing code while still supporting the new functionality of mixing Tools and Toolsets.

    Users who encountered type errors like "Argument of type 'list\[Tool\]' cannot be assigned to parameter 'tools'" should no longer see these errors after upgrading. No code changes are required on the user side.

-   When creating a pipeline snapshot we make sure to use \_deepcopy_with_exceptions when copying component inputs to avoid deep copies of items like components and tools since they often contain attributes that are not deep-copyable. For example, the LinkContentFetcher has httpx.Client as an attribute which throws an error if we try to deep copy it.

## v2.20.0-rc0

### Highlights

Introduced <span class="title-ref">FallbackChatGenerator</span> that tries multiple chat providers one by one, improving reliability in production and making sure you get answers even when some provider fails.

### New Features

-   Updated our serialization and deserialization of PipelineSnapshots to work with python Enum classes.

-   Added <span class="title-ref">FallbackChatGenerator</span> that automatically retries different chat generators and returns first successful response with detailed information about which providers were tried.

-   -   Added <span class="title-ref">pipeline_snapshot</span> and <span class="title-ref">pipeline_snapshot_file_path</span> parameters to <span class="title-ref">BreakpointException</span> to provide more context when a pipeline breakpoint is triggered.
    -   Added <span class="title-ref">pipeline_snapshot_file_path</span> parameter to <span class="title-ref">PipelineRuntimeError</span> to include a reference to the stored pipeline snapshot so it can be easily found.

-   A new component <span class="title-ref">RegexTextExtractor</span> which allows to extract text from chat messages or strings input based on custom regex pattern.

-   CSVToDocument: add <span class="title-ref">conversion_mode='row'</span> with optional <span class="title-ref">content_column</span>; each row becomes a <span class="title-ref">Document</span>; remaining columns stored in <span class="title-ref">meta</span>; default 'file' mode preserved.

-   Added the ability to resume an <span class="title-ref">Agent</span> from an <span class="title-ref">AgentSnapshot</span> while specifying a new breakpoint in the same run call. This allows stepwise debugging and precise control over chat generator inputs tool inputs before execution, improving flexibility when inspecting intermediate states. This addresses a previous limitation where passing both a snapshot and a breakpoint simultaneously would throw an exception.

-   Introduce <span class="title-ref">SentenceTransformersSparseTextEmbedder</span> and <span class="title-ref">SentenceTransformersSparseDocumentEmbedder</span> components. These components embed text and documents using sparse embedding models compatible with Sentence Transformers. Sparse embeddings are interpretable, efficient when used with inverted indexes, combine classic information retrieval with neural models, and are complementary to dense embeddings. Currently, the produced <span class="title-ref">SparseEmbedding</span> objects are compatible with the <span class="title-ref">QdrantDocumentStore</span>.

    Usage example: `` `python from haystack.components.embedders import SentenceTransformersSparseTextEmbedder ``\` text_embedder = SentenceTransformersSparseTextEmbedder() text_embedder.warm_up()

    print(text_embedder.run("I love pizza!"))

    \# {'sparse_embedding': SparseEmbedding(indices=\[999, 1045, ...\], values=\[0.918, 0.867, ...\])} \`\`\`

-   Added a <span class="title-ref">warm_up()</span> function to the <span class="title-ref">Tool</span> dataclass, allowing tools to perform resource-intensive initialization before execution. Tools and Toolsets can now override the <span class="title-ref">warm_up()</span> method to establish connections to remote services, load models, or perform other preparatory operations. The <span class="title-ref">ToolInvoker</span> and <span class="title-ref">Agent</span> automatically call <span class="title-ref">warm_up()</span> on their tools during their own warm-up phase, ensuring tools are ready before use.

### Enhancement Notes

-   Added <span class="title-ref">tools</span> to agent run parameters to enhance the agent's flexibility. Users can now choose a subset of tools for the agent at runtime by providing a list of tool names, or supply an entirely new set by passing Tool objects or a Toolset.
-   Enhanced the tools parameter across all tool-accepting components (Agent, ToolInvoker, OpenAIChatGenerator, AzureOpenAIChatGenerator, HuggingFaceAPIChatGenerator, HuggingFaceLocalChatGenerator) to accept either a mixed list of Tool and Toolset objects or just a Toolset object. Previously, components required either a list of Tool objects OR a single Toolset, but not both in the same list. Now users can organize tools into logical Toolsets while also including standalone Tool objects, providing greater flexibility in tool organization. For example: <span class="title-ref">Agent(chat_generator=generator, tools=\[math_toolset, weather_toolset, standalone_tool\])</span>. This change is fully backward compatible and preserves structure during serialization/deserialization, enabling proper round-trip support for mixed tool configurations.
-   Refactored <span class="title-ref">\_save_pipeline_snapshot</span> to consolidate try-except logic and added a <span class="title-ref">raise_on_failure</span> option to control whether save failures raise an exception or are logged. <span class="title-ref">\_create_pipeline_snapshot</span> now wraps <span class="title-ref">\_serialize_value_with_schema</span> in try-except blocks to prevent failures from non-serializable pipeline inputs.

### Bug Fixes

-   Fix Agent <span class="title-ref">run_async</span> method to correctly handle async streaming callbacks. This previously triggered errors due to a bug.
-   Prevent duplication of the last assistant message in the chat history when initializing from an <span class="title-ref">AgentSnapshot</span>.
-   We were setting response_format to None in OpenAIChatGenerator by default which doesn't follow the API spec. We now omit the variable if response_format is not passed by the user.
-   Ensure that the <span class="title-ref">OpenAIChatGenerator</span> is properly serialized when <span class="title-ref">response_format</span> in <span class="title-ref">generation_kwargs</span> is provided as a dictionary (for example, <span class="title-ref">{"type": "json_object"}</span>). Previously, this caused serialization errors.
-   Fixed parameter schema generation in <span class="title-ref">ComponentTool</span> when using <span class="title-ref">inputs_from_state</span>. Previously, parameters were only removed from the schema if the state key and parameter name matched exactly. For example, <span class="title-ref">inputs_from_state={"text": "text"}</span> removed <span class="title-ref">text</span> as expected, but <span class="title-ref">inputs_from_state={"state_text": "text"}</span> did not. This is now resolved, and such cases work as intended.
-   Refactored <span class="title-ref">SentenceTransformersEmbeddingBackend</span> to ensure unique embedding IDs by incorporating all relevant arguments.
-   Fixed Agent to correctly raise a <span class="title-ref">BreakpointException</span> when a <span class="title-ref">ToolBreakpoint</span> with a specific <span class="title-ref">tool_name</span> is provided in an assistant chat message containing multiple tool calls.
-   The <span class="title-ref">OpenAIChatGenerator</span> implementation uses <span class="title-ref">ChatCompletionMessageCustomToolCall</span>, which is only available in OpenAI client \>=1.99.2. We now require <span class="title-ref">openai\>=1.99.2</span>.

